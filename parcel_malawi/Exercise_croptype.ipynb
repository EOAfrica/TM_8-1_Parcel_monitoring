{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this notebook, we'll start from a time series of Sentinel-2 data, as well as a in-situ database of crop types in Malawi to train a crop type mapping algorithm.\n",
    "\n",
    "By using the prior knowledge obtained from the previous exercise, we will preprocess the data, extract the data at our in-situ data locations, perform feature extraction and train a Random Forest Classifier.\n",
    "\n",
    "First, let's explore the in-situ data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Explore the in-situ data\n",
    "\n",
    "By loading our in-situ dataframe, we can assess the variety of the dataset, as well as the location of the fields.\n",
    "\n",
    "\n",
    "To start, load the in-situ dataset and visualize the amount and variety of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "inpath = Path('/data/users/Public/couchard/Malawi_normalized_data_2023.gpkg')\n",
    "croptype_df = <YOUR CODE HERE>\n",
    "\n",
    "# Visualize the first few rows\n",
    "croptype_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we count the amount of samples for each different croptype\n",
    "value_counts = croptype_df['croptype'].value_counts()\n",
    "\n",
    "# Let's make a bar plot to show the amount of samples for each croptype\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is healthy amount of samples for Soy beans, Three crops, Maize, Oilseed and Tobacco. The other croptypes are too scarse in amount of points. When there are not enough data samples, it can be difficult to train a classifier.\n",
    "\n",
    "Select only the first 4 most dominant crop types, and rename the other croptypes to the \"Other\" class. Do not consider the class, \"Mixture of threes or more\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "croptype_df = <YOUR CODE HERE>\n",
    "croptype_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you we have our filterered dataset, evaluate the bounding box of the points.\n",
    "\n",
    "Using GeoPandas plotting functionalities, create an bounding box enveloping the entirety of our training dataset. To give it a comparison, plot it against the country of Malawi.\n",
    "\n",
    "Documentation for those functionalities is available here:\n",
    "https://geopandas.org/en/stable/docs/user_guide/mapping.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a dataset containing all the countries borders, and search for Malawi.\n",
    "# Plot the malawi border and adds the bounding box envelopping all our training data points on the plot.\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load and preprocess the training data\n",
    "\n",
    "Using the same Sentinel-2 pre-processing piepeline used in the previous exercise, load the data and perform the same pre-processing operations.\n",
    "\n",
    "* Compute the cloud mask from the 'SCENECLASSIFICATION' layer.\n",
    "* Load the 20-m array, mask the clouds, perform 10-daily temporal compositing and linear interpolation.\n",
    "* Using the same cloud mask, do the same cloud masking, temporal compositing and interpolation with the 10-m array.\n",
    "\n",
    "No need to upsample the 20-m array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python libraries...\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import custom python functions all defined in separate .py files\n",
    "# custom functions\n",
    "from mask import mask_ts\n",
    "from composite import composite_ts\n",
    "from interpolate import interpolate_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_20m = '/data/users/Public/couchard/S2_L2A_Malawi_20m.nc'\n",
    "infile_10m = '/data/users/Public/couchard/S2_L2A_Malawi_10m.nc'\n",
    "\n",
    "ds_20 = xr.open_dataset(infile_20m)\n",
    "\n",
    "ts_20_fin = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_10 = xr.open_dataset(infile_10m)\n",
    "\n",
    "ts_10_fin = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the NDVI value, as done in the previous exercise.\n",
    "\n",
    "Then, using the <b>xr.concat(...)</b> function, assemble the two preprocessed arrays and the NDVI into a single array.\n",
    "\n",
    "Docs: https://docs.xarray.dev/en/stable/user-guide/combining.html#concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_fin = xr.concat([ts_10_fin, ts_20_fin], dim='variable')\n",
    "\n",
    "ndvi = <YOUR CODE HERE>\n",
    "\n",
    "# Concatenate the ndvi with the other bands, forming an entire array\n",
    "ts_fin = <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can free memory from your compute by removing all the intermediate arrays used for cloud masking, compositing, linear interpolation...\n",
    "del <YOUR INTERMEDIARY ARRAYS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train a crop type classifier using Machine Learning.\n",
    "\n",
    "In order to create a crop type map in malawi, we first need to train a Random Forest Classifier with the in-situ data we have available.\n",
    "\n",
    "In this section, you will have to:\n",
    "\n",
    "* Sample points within the training data we have at our disposition\n",
    "* For every point, combine the variable and temporal dimension in to a single dimension using quentiles/percentiles.\n",
    "* Train de model, evaluate its performance and perform full-tile inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample random points from the GeoPandas dataset\n",
    "\n",
    "From the GeoPandas dataset of croptypes constructed previously in this exercise, sample random points using the `geopandas.GeoSeries.sample_points` function.\n",
    "\n",
    "Then, sample those points from the array that we just assembled\n",
    "\n",
    "Docs: https://geopandas.org/en/stable/docs/user_guide/sampling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POINTS_PER_FIELD = 10\n",
    "\n",
    "sampled_points = <YOUR CODE HERE>\n",
    "\n",
    "# Prepare the point labels so \n",
    "point_labels = croptype_df.croptype.repeat(POINTS_PER_FIELD)\n",
    "\n",
    "# Visualize the sampled points against the croptype dataset\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `extract_points` function, extract the sampled points from the `ts_fin` sensor array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract import extract_points\n",
    "\n",
    "training_data = <YOUR CODE HERE>\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute additional features using percentiles\n",
    "\n",
    "One of the most interesting aspect of Remote Sensing when using Machine Learning is the temporal dimension. Different objects visible on the surface of earth will have a different evolution of with time. The most notable examples are usually in vegetation, were seasonal properties exists depending on the region and the species. Looking at the temporal dimension can therefore improve classification tasks. Recent research push Deep Learning techniques to analyse those temporal characteristics and use the most of it.\n",
    "\n",
    "Unfortunately, analyzing the temporal dimension obviously implies the use of another dimension, increasing tremendously the computational power requirements. This does not help the realization of large Remote Sensing projects, which already suffer from the very large quantities of data to process.\n",
    "\n",
    "There is a large variety of techniques that can be used to mitigate the issue by combining the `variable` and the `time` dimensions into one. Here, we will use percentiles/quantiles computed on the `time` dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `band_percentile` function, extract the following percentiles:\n",
    "\n",
    "* For the Red, Green, Blue and NIR bands, we compute the 10th, 50th and 90th percentiles.\n",
    "* For the NDVI, we compute the 10th, 25th, 50th, 75th and 90th percentiles.\n",
    "* For the other bands, we only keep the 50th percentile (median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import band_percentile\n",
    "\n",
    "band_percentiles = {\n",
    "    'B02': [0.1, 0.5, 0.9],\n",
    "    'B03': [0.1, 0.5, 0.9],\n",
    "    'B04': [0.1, 0.5, 0.9],\n",
    "    'B08': [0.1, 0.5, 0.9],\n",
    "    'NDVI': [0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "    'B05': [0.5],\n",
    "    'B06': [0.5],\n",
    "    'B07': [0.5],\n",
    "    'B11': [0.5],\n",
    "    'B12': [0.5]\n",
    "}\n",
    "\n",
    "training_data_df = band_percentile(training_data, band_percentiles).to_dataframe(\n",
    "        name='features', dim_order=['sample', 'features']\n",
    ").unstack()\n",
    "\n",
    "training_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset ⚖️\n",
    "\n",
    "Because the \"Other\" class is very dominant in our dataset, we will use a small feature from the `imbalance-learn` package, allowing to better equilibrate the dataset and have the same number of class for each croptype.\n",
    "\n",
    "Having an equilibrated dataset is important as it prevents the model to overfit on a certain class.\n",
    "\n",
    "Initialize a `RandomUnderSampler` object and perform resampling on the `train_data_df` and `point_labels` datasets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X, y = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Time 💃 Let's train a Random Forest Classifier\n",
    "\n",
    "Using the sklearn framework, let's train a random forest classifier. Before fitting the data within our model, we still need to do a final step: separating the data in two sets: the <b>training</b> data and the <b>validation</b> data. In order to prove that the model generalizes well on new samples, we want to test the performance of our model on different data than the one that was used to train it.\n",
    "\n",
    "This split can be easily done from our DataFrame using the `sklearn.model_selection.train_test_split` function.\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Once split the training dataset split, train the model with it\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Let's perform dataset split\n",
    "X_train, X_test, y_train, y_test = <YOUR CODE HERE>\n",
    "\n",
    "# Initialize the model, you can use the default model parameters defined in Sklearn, as they are usually very good\n",
    "model = <YOUR CODE HERE>\n",
    "\n",
    "# We train our model\n",
    "<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evoluate model performance\n",
    "\n",
    "To evaluate the model performance, we perform prediction on the validation data that we previously split from. Then, we can use the `sklearn.metrics.accuracy_score` and `sklearn.metrics.confusion_matrix` functions.\n",
    "\n",
    "The accuracy score denotates how many validation examples were correctly labelled by the model, while the confusion matrix shows where the prediction errors are made by comparing in-situ crop types with crop types predicted by the model.\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# First, we predict the result the model on the validation data\n",
    "y_pred = <YOUR CODE HERE>\n",
    "\n",
    "# Compute the accuracy score, the confusion matrix and the plot the confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "\n",
    "# Set labels, title, and ticks\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(np.arange(len(cm)) + 0.5, labels=model.classes_, rotation=30)  # Replace labels with your class names\n",
    "plt.yticks(np.arange(len(cm)) + 0.5, labels=model.classes_, rotation=30)  # Replace labels with your class names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference for crop type mapping 🗺️\n",
    "\n",
    "Now that our model is trained, let's perform inference on the entirety of our downloaded data.\n",
    "\n",
    "First, compute the same quantiles as done for the training data, but this time on the entire data cube instead of the samples datacube. Then combine those quantile cubes into a single one using the xr.concat method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_inference = <YOUR CODE HERE>\n",
    "\n",
    "inference_df = ts_inference.to_dataframe(\n",
    "    name='features', dim_order=['x', 'y', 'features']\n",
    ").unstack()\n",
    "\n",
    "inference_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done previously, transform the entire datacube into a single dataset, then perform model inference using the `model.predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = <YOUR CODE HERE>\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map visualization 🔎\n",
    "\n",
    "Let's reconstruct the map from the given prediction. Let's map every class to a tuple giving a color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blue\n",
    "OTHER_COLOR = (48, 110, 209)\n",
    "\n",
    "# Yellow\n",
    "MAIZE = (204, 164, 55)\n",
    "\n",
    "# Purple\n",
    "SOY_BEANS = (163, 5, 158)\n",
    "\n",
    "# Olive color\n",
    "OILSEEDS_CROPS = (219, 245, 91)\n",
    "\n",
    "# Brown\n",
    "TOBACCO = (107, 55, 55)\n",
    "\n",
    "color_map = {\n",
    "    'tobacco': TOBACCO,\n",
    "    'oilseed_crops': OILSEEDS_CROPS,\n",
    "    'soy_soybeans': SOY_BEANS,\n",
    "    'Other': OTHER_COLOR,\n",
    "    'maize': MAIZE\n",
    "}\n",
    "\n",
    "def map_class_to_color(class_value: str):\n",
    "    # Returns the color of the given class, and black color if unknown\n",
    "    return color_map.get(class_value, (0, 0, 0))\n",
    "\n",
    "vectorized_map = np.vectorize(map_class_to_color)\n",
    "\n",
    "rgb_array = np.array(vectorized_map(prediction)).squeeze().reshape(\n",
    "    3, ts_inference.shape[1], ts_inference.shape[0]\n",
    ")\n",
    "\n",
    "rgb_array = np.moveaxis(rgb_array, 0, -1)\n",
    "\n",
    "prediction_array = xr.DataArray(\n",
    "    rgb_array,\n",
    "    dims=['x', 'y', 'channels'],\n",
    "    coords={\n",
    "        'x': ts_inference.coords['x'],\n",
    "        'y': ts_inference.coords['y'],\n",
    "        'channels': ['R', 'G', 'B']\n",
    "    }\n",
    ")\n",
    "\n",
    "prediction_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(ncols=2, nrows=1, figsize=(12, 6))\n",
    "\n",
    "rgb = (ts_inference.sel(features=['B04-0.5', 'B03-0.5', 'B02-0.5']) / 1e4) ** .4\n",
    "\n",
    "axis[0].imshow(rgb)\n",
    "axis[1].imshow(rgb_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "\n",
    "sensor_layer = rgb.hvplot.rgb(x='x', y='y', bands='features', aspect=1, width=500)\n",
    "prediction_layer = prediction_array.transpose('channels','y', 'x').hvplot.rgb(x='x', y='y', bands='channels', aspect=1, width=500)\n",
    "\n",
    "sensor_layer + prediction_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
