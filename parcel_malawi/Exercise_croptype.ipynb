{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/crop_type.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this notebook, we'll start from a time series of Sentinel-2 data, as well as an in-situ database of crop types in Malawi to train a crop type mapping algorithm.\n",
    "\n",
    "By using the prior knowledge obtained from the previous exercise, we will preprocess the data, extract the data at our in-situ data locations, perform feature extraction and train a Random Forest Classifier.\n",
    "\n",
    "First, let's explore the in-situ data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Explore the in-situ data\n",
    "\n",
    "By loading our in-situ dataframe, we can assess the variety of the dataset, as well as the location of the fields.\n",
    "\n",
    "\n",
    "To start, load the in-situ dataset and visualize the amount and variety of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "datadir = Path('./data')\n",
    "shapefile = str(datadir / 'Malawi_normalized_data_2023.gpkg')\n",
    "croptype_df = <YOUR CODE HERE>\n",
    "\n",
    "# Visualize the first few rows\n",
    "croptype_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the dataset contains all essential components of crop type in-situ data:\n",
    "- sampleID -> a unique ID for each individual observation\n",
    "- validityTi -> an observation date\n",
    "- croptype -> the crop type label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we count the amount of samples for each different croptype\n",
    "value_counts = croptype_df['croptype'].value_counts()\n",
    "\n",
    "# Let's make a bar plot to show the amount of samples for each croptype\n",
    "fig = plt.barh(value_counts.index, value_counts.values, color='purple', height=0.4)\n",
    "plt.xlabel('Occurences per crop type in in-situ data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is healthy amount of samples for Soy beans, mixture of three or more crops, Maize, Oilseed and Tobacco. The other croptypes are too scarse in amount of points. When there are not enough data samples, it can be difficult to train a classifier.\n",
    "\n",
    "Select only the first 4 most dominant crop types, and rename the other crop types to the \"Other\" class. Do not consider the class, \"Mixture of three or more\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the 4 most common crop types in the data\n",
    "croptypes = list(value_counts.sort_values(ascending=False).index)[:5]\n",
    "\n",
    "# Now convert all crop types not part of the list to \"other\"\n",
    "< your code here >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert all crop types not part of the list to \"other\"\n",
    "def filter_value(val: str) -> str:\n",
    "    if val in croptypes:\n",
    "        return val\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "croptype_df['croptype'] = croptype_df.croptype.apply(filter_value)\n",
    "croptype_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our filterered dataset, evaluate the bounding box of the points.\n",
    "\n",
    "Using GeoPandas plotting functionalities, create a bounding box enveloping the entirety of our training dataset. To give it a comparison, plot it against the country of Malawi.\n",
    "\n",
    "Documentation for those functionalities is available here:\n",
    "https://geopandas.org/en/stable/docs/user_guide/mapping.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a dataset containing all the country borders, and search for Malawi\n",
    "country_df = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "malawi = country_df[country_df.name == 'Malawi']\n",
    "\n",
    "# Compute the envelope\n",
    "west, south, east, north = croptype_df.total_bounds\n",
    "from shapely.geometry import box\n",
    "bounds_df = gpd.GeoDataFrame(geometry=[box(west, south, east, north)], crs=country_df.crs)\n",
    "\n",
    "# Show the country borders and the envelope\n",
    "base = malawi.plot(color='pink', figsize=(12, 6))\n",
    "bounds_df.plot(ax=base)\n",
    "base.set_xlabel('Malawi (pink) and the region of the training data (blue)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have any comment on the extent of this training dataset? What does it imply for the applicability of our crop type model?\n",
    "\n",
    "Why do you think we need the bounding box of the dataset??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load and preprocess the training data\n",
    "\n",
    "We have downloaded the Sentinel-2 data you will need for this exercise for you already.\n",
    "\n",
    "Using the same Sentinel-2 pre-processing pipeline used in the previous exercise, load the data and perform the same pre-processing operations.\n",
    "\n",
    "* Compute the cloud mask from the 'SCENECLASSIFICATION' layer.\n",
    "* Load the 20-m array, mask the clouds, perform 10-daily temporal compositing and linear interpolation.\n",
    "* Using the same cloud mask, do the same cloud masking, temporal compositing and interpolation with the 10-m array.\n",
    "\n",
    "No need to upsample the 20-m array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary python libraries...\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import custom python functions all defined in separate .py files\n",
    "# custom functions\n",
    "from mask import mask_ts\n",
    "from composite import composite_ts\n",
    "from interpolate import interpolate_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_20m = str(datadir / 'S2_L2A_Malawi_20m.nc')\n",
    "infile_10m = str(datadir / 'S2_L2A_Malawi_10m.nc')\n",
    "\n",
    "ds_20 = xr.open_dataset(infile_20m)\n",
    "\n",
    "<YOUR CODE HERE>\n",
    "\n",
    "ts_20_fin = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_10 = xr.open_dataset(infile_10m)\n",
    "\n",
    "<YOUR CODE HERE>\n",
    "\n",
    "ts_10_fin = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the NDVI value, as done in the previous exercise.\n",
    "\n",
    "Then, using the <b>xr.concat(...)</b> function, assemble the two preprocessed arrays and the NDVI into a single array.\n",
    "\n",
    "Docs: https://docs.xarray.dev/en/stable/user-guide/combining.html#concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_fin = xr.concat([ts_10_fin, ts_20_fin], dim='variable')\n",
    "\n",
    "ts_B08 = xxx\n",
    "ts_B04 = xxx\n",
    "ndvi = xxx\n",
    "# convert ndvi to proper data array\n",
    "ndvi = ndvi.expand_dims(dim='variable', axis=0).assign_coords({'variable': ['NDVI']})\n",
    "\n",
    "# Concatenate the ndvi with the other bands\n",
    "ts_fin = <YOUR CODE>\n",
    "ts_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can free memory from your computer by removing all the intermediate arrays used for cloud masking, compositing, linear interpolation...\n",
    "del <YOUR INTERMEDIARY ARRAYS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train a crop type classifier using Machine Learning.\n",
    "\n",
    "In order to create a crop type map in malawi, we first need to train a Random Forest Classifier with the in-situ data we have available.\n",
    "\n",
    "In this section, you will have to:\n",
    "\n",
    "* Sample points within the training data we have at our disposition\n",
    "* For every point, combine the variable and temporal dimension into a single dimension using quantiles/percentiles.\n",
    "* Train the model and evaluate its performance \n",
    "* Perform full-tile inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample random points from the GeoPandas dataset\n",
    "\n",
    "From the GeoPandas dataset of croptypes constructed previously in this exercise, sample random points using the `geopandas.GeoSeries.sample_points` function.\n",
    "\n",
    "Then, sample those points from the array that we just assembled\n",
    "\n",
    "Docs: https://geopandas.org/en/stable/docs/user_guide/sampling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POINTS_PER_FIELD = 10\n",
    "\n",
    "# use the sample_points function on the geometry component of the crop type dataframe\n",
    "sampled_points = xxx.sample_points(POINTS_PER_FIELD, method='cluster_poisson')\n",
    "\n",
    "# Prepare the point labels\n",
    "point_labels = croptype_df.croptype.repeat(POINTS_PER_FIELD)\n",
    "\n",
    "# Visualize the sampled points against the original dataset\n",
    "fig, axis = plt.subplots(1, 1, figsize=(6, 6))\n",
    "croptype_df.plot(ax=axis, legend=True)\n",
    "xxx.plot(ax=axis, c='red', markersize=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `extract_points` function, extract the sampled points from the `ts_fin` sensor array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract import extract_points\n",
    "\n",
    "training_data = <YOUR CODE HERE>\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute additional features using percentiles\n",
    "\n",
    "One of the most interesting aspects of satellite data when using Machine Learning is the temporal dimension. Different objects visible on the surface of the earth will have a different evolution with time. The most notable examples are usually in vegetation, where seasonal properties exist depending on the region and the species. Looking at the temporal dimension can therefore improve classification tasks. Recent research pushes deep/machine Learning techniques to analyse those temporal characteristics and use the most of it.\n",
    "\n",
    "Unfortunately, including the full temporal dimension of the data increases tremendously the computational power requirements. This does not help the realization of large Remote Sensing projects, which already suffer from the very large quantities of data to process.\n",
    "\n",
    "There is a large variety of techniques that can be used to mitigate the issue by combining the `variable` and the `time` dimensions into one dimension. Here, we will use percentiles/quantiles computed on the `time` dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `band_percentile` function, extract the following percentiles:\n",
    "\n",
    "* For the Red, Green, Blue and NIR bands, we compute the 10th, 50th and 90th percentiles.\n",
    "* For the NDVI, we compute the 10th, 25th, 50th, 75th and 90th percentiles.\n",
    "* For the other bands, we only keep the 50th percentile (median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import band_percentile\n",
    "\n",
    "# Here we specify which percentiles need to be computed for each of the bands available\n",
    "band_percentiles = {\n",
    "    'B02': [0.1, 0.5, 0.9],\n",
    "    'B03': [0.1, 0.5, 0.9],\n",
    "    'B04': [0.1, 0.5, 0.9],\n",
    "    'B08': [0.1, 0.5, 0.9],\n",
    "    'NDVI': [0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "    'B05': [0.5],\n",
    "    'B06': [0.5],\n",
    "    'B07': [0.5],\n",
    "    'B11': [0.5],\n",
    "    'B12': [0.5]\n",
    "}\n",
    "\n",
    "training_data_df = band_percentile(training_data, band_percentiles).to_dataframe(\n",
    "        name='features', dim_order=['sample', 'features']\n",
    ").unstack()\n",
    "\n",
    "training_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset ⚖️\n",
    "\n",
    "Because the \"Other\" class is very dominant in our dataset, we will use a small feature from the `imbalance-learn` package, allowing to better equilibrate the dataset and have the same number of samples for each crop type.\n",
    "\n",
    "Having an equilibrated dataset is important as it prevents the model to overfit on a certain class.\n",
    "\n",
    "Initialize a `RandomUnderSampler` object and perform resampling on the `training_data_df` and `point_labels` datasets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X, y = undersampler.fit_resample(xxx, xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Time 💃 Let's train a Random Forest Classifier\n",
    "\n",
    "Using the sklearn framework, let's train a random forest classifier. Before fitting the data within our model, we still need to do a final step: separating the data in two sets: the <b>training</b> data and the <b>validation</b> data. In order to prove that the model generalizes well on new samples, we want to test the performance of our model on different data than the one that was used to train it.\n",
    "\n",
    "This split can be easily done from our DataFrame using the `sklearn.model_selection.train_test_split` function.\n",
    "\n",
    "Split your dataset in 80% training and 20% validation data!\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Once the training dataset has been generated, train the model with it.\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Let's perform dataset split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=xxx, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the model, you can use the default model parameters defined in Sklearn, as they are usually very good\n",
    "model = CatBoostClassifier()\n",
    "\n",
    "# We train our model\n",
    "model.fit(xxx, xxx, logging_level='Silent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance\n",
    "\n",
    "To evaluate the model performance, we perform prediction on the validation data that we previously generated during dataset splitting. Then, we can use the `sklearn.metrics.accuracy_score` and `sklearn.metrics.confusion_matrix` functions.\n",
    "\n",
    "The accuracy score denotes how many validation examples were correctly labelled by the model, while the confusion matrix shows where the prediction errors are made by comparing the real crop type labels with the crop type labels predicted by the model.\n",
    "\n",
    "Docs: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# First, we predict the result the model on the validation data\n",
    "y_pred = model.predict(xxx)\n",
    "\n",
    "# Compute the accuracy score and the confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, xxx)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "\n",
    "# Set labels, title, and ticks\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(np.arange(len(cm)) + 0.5, labels=model.classes_, rotation=30)  # Replace labels with your class names\n",
    "plt.yticks(np.arange(len(cm)) + 0.5, labels=model.classes_, rotation=30)  # Replace labels with your class names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference for crop type mapping 🗺️\n",
    "\n",
    "Now that our model is trained, let's perform inference on the entirety of our downloaded data.\n",
    "\n",
    "First, compute the same quantiles as done for the training data, but this time on the entire data cube instead of the samples datacube.\n",
    "\n",
    "To speed up processing, we use dask to chunck the data in small pieces and run the processing in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.diagnostics.progress import ProgressBar\n",
    "\n",
    "dask.config.set(scheduler='multiprocessing')\n",
    "\n",
    "with ProgressBar():\n",
    "    ts_fin = ts_fin.chunk({'x': 100, 'y': 100, 'timestamp': -1, 'variable': -1})\n",
    "    ts_inference = band_percentile(xxx, band_percentiles).compute()\n",
    "\n",
    "    inference_df = ts_inference.to_dataframe(\n",
    "        name='features', dim_order=['x', 'y', 'features']\n",
    "    ).unstack()\n",
    "\n",
    "inference_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform model inference using the `model.predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = <YOUR CODE HERE>\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map visualization 🔎\n",
    "\n",
    "Let's reconstruct the map from the given prediction. Let's map every class to a color..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blue\n",
    "OTHER_COLOR = (48, 110, 209)\n",
    "\n",
    "# Yellow\n",
    "MAIZE = (204, 164, 55)\n",
    "\n",
    "# Purple\n",
    "SOY_BEANS = (163, 5, 158)\n",
    "\n",
    "# Olive color\n",
    "OILSEEDS_CROPS = (219, 245, 91)\n",
    "\n",
    "# Brown\n",
    "TOBACCO = (107, 55, 55)\n",
    "\n",
    "color_map = {\n",
    "    'tobacco': TOBACCO,\n",
    "    'oilseed_crops': OILSEEDS_CROPS,\n",
    "    'soy_soybeans': SOY_BEANS,\n",
    "    'Other': OTHER_COLOR,\n",
    "    'maize': MAIZE\n",
    "}\n",
    "\n",
    "def map_class_to_color(class_value: str):\n",
    "    # Returns the color of the given class, and black color if unknown\n",
    "    return color_map.get(class_value, (0, 0, 0))\n",
    "\n",
    "vectorized_map = np.vectorize(map_class_to_color)\n",
    "\n",
    "rgb_array = np.array(vectorized_map(prediction)).squeeze().reshape(\n",
    "    3, ts_inference.shape[1], ts_inference.shape[0]\n",
    ")\n",
    "\n",
    "rgb_array = np.moveaxis(rgb_array, 0, -1)\n",
    "\n",
    "prediction_array = xr.DataArray(\n",
    "    rgb_array,\n",
    "    dims=['x', 'y', 'channels'],\n",
    "    coords={\n",
    "        'x': ts_inference.coords['x'],\n",
    "        'y': ts_inference.coords['y'],\n",
    "        'channels': ['R', 'G', 'B']\n",
    "    }\n",
    ")\n",
    "\n",
    "prediction_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(ncols=2, nrows=1, figsize=(12, 6))\n",
    "\n",
    "rgb = (ts_inference.sel(features=['B04-0.5', 'B03-0.5', 'B02-0.5']) / 1e4) ** .4\n",
    "\n",
    "axis[0].imshow(rgb)\n",
    "axis[1].imshow(rgb_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
